# Causal Multi-Agent Reinforcement Learning: Review and Open Problems

## 摘要

这篇文章主要是讲了 多智能体强化学习 和 因果关系研究方法 的 交集 

## 介绍 

珀尔认为显式因果对实现通用智能十分重要，因果关系可以提供更好的可解释性。  
传统机器学习纯将观察数据用于回归和分类，在强化学习中，尽管更够在环境中执行干预(即采取action)，
但没有正式的因果模型，RL智能体缺乏明确的反事实推理的能力。
通过重新制定RL模型/范式，将RL和来自因果的方法结合起来，有一些在特定任务特定情况下的研究，比如off-policy学习，data-fusion
和反事实推理。本文考虑将因果强化学习方法扩展到多智能体情况，其中由于交互代理而产生了额外的复杂性。此外，它假定因果工具为解决其中一些挑战提供了适当的属性。

强化学习，强化学习的问题关注如何采取行为最大化奖励值，一个强化学习的智能体通过
试错来学习去选择优化行为的序列。奖励的信号可以被部分观测、噪音、或者较多数量的因子混杂。环境的随机性或非平稳性会加剧学习问题的难度。
马尔科夫决策过程定义了一个用于顺序决策场景的有用模型。

DRT(dynamic treatment regime),在DRT中，医生应利用所有可用信息来优化患者的长期治疗效果。
由于存在影响医护人员可以访问的变量的潜在（未观察到的）因素，DTR 变得更加困难。 在患者结果的情况下，由于系统的复杂性，无法知道干预措施在很长一段时间内对个人产生的确切影响。
 例如，多疗程化疗治疗计划导致的缓解可能取决于初始治疗（Wang 等，2012）。 这种关系可以由未知的因果因素决定，在这种情况下，没有一个状态会满足马尔可夫假设。
一般而言，系统中可能存在有关变量的信息或先验知识，这些变量在假设之上形成模型。
我们现在讨论在复杂交互和假设的系统中形式化因果推理的方法。

因果关系。因果推理考虑了如何以及何时可以从数据中得出因果结论。
最近，人们对在建模过程中明确因果假设所带来的好处产生了极大的兴趣。
一般来说，相互作用变量的复杂系统可以用结构因果模型 (SCM) 来描述 (Pearl, 2009; Peters et al., 2017)。这样的模型描述了任意系统中存在的因果机制和假设。
变量之间的关系可以用因果贝叶斯网络 (CBN) 的形式图形化地表示，其中节点代表变量，有向路径代表变量之间的因果影响（见图 1）。
读者不应被源于因果建模的结构方程建模 (SEM) 中的相关工作所迷惑（Pearl，2012 年）。 Pearl (1998) 通过解释此类方法何时对声称因果结果有效，澄清了对 SEM 中因果假设的混淆。彼得斯等人。 
（2017）将SCM制定如下：
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
